\documentclass[11pt,a4paper]{article}
%PKG
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{subfig}
\usepackage[a4paper, total={6in, 9in}]{geometry}

%PARAM
%Linux
%\graphicspath{{media/}}
%Windows
\graphicspath{{./media/}}

%FUNCTIONS
\newcommand{\centerFigure}[2]{
\begin{figure}[ht]	
%Image: #1
\centering
\includegraphics[width=10cm]{#1}
\caption{#2}
\end{figure}
}

%DOCUMENT
\begin{document}
\pagestyle{headings}
\author{Axel Jeanne \and Monica Arias Rivera}
\title{Cooperative Supervised Mobile Manipulator}
\maketitle
\begin{abstract}
Robotic systems are growing in complexity and diversity due to the increasing challenges of the tasks
they are performing. As complexity increases, decoupling elements into smaller simpler parts is
regularly done. Following this decoupling trend, cooperative robotics grants an even 
larger change: using multiple robots to perform a single, yet complex, task.

This report present the work performed during the Supervised Mobile Manipulator (SMM) project.
The project consists in making a cooperative robotic system provide ``smart" behaviour to
make the tele-operation easier to the user, while keeping effectiveness of the controlled robots 
unchanged.

For this project we used different hardware and software elements that we will introduce,
then we will present different cooperative behaviours and will expose their strengths and weaknesses; finally we will present the chosen system and expose its features and the results we obtained.
\end{abstract}
\clearpage

\tableofcontents

\clearpage
\section{Theoretical content}
\subsection{Advantages of cooperative system}
In most cases, the operator performing a task remotely only has a single point of view: the
one of the embedded camera. In multiple different systems, this can
lead to inconvenient situations where the operator is performing a task with a sub-optimal
vision of the situation.

One solution to this problem could be to add an animate vision system \cite{Ballard1991}
but this kind of solution vastly increases the complexity of the system since the whole vision system
has to be actuated. Another solution could be to increase the number of cameras on the robot.
This solution increases the robot payload and is actually not very convenient for the user
since more video streams now have to be monitored while performing the task. The solution we chose is to add an external robot to provide a ``supervisor" vision of the acting robot. This solution adds many different  advantages:

\begin{enumerate}
\item The supervisor can provide vision from different angles while the action robot is static
\item The supervisor can scout ahead of the action robot to ensure safety
\item The supervisor behaviours can be automated to unload the operator's attention
\end{enumerate}


\subsection{Problem statement}
The Supervised Mobile Manipulator Project (SMM) goal is to develop a cooperative robotics
system which can be used in hazardous areas. It uses two platforms: 

\begin{itemize}
\item An action platform which perform the physical operation
\item A supervisor platform which keeps an overview of the situation
\end{itemize}

\centerFigure{teleopSystem.png}{The tele-operated robotic system}

To deliver a fully applicable solution, a GUI (Graphical User Interface) is provided in
order to see what the system is doing in real time.


\subsection{Tools used}
\subsubsection{Git}
To deliver a complete solution, a lot of software development was required. In order to keep
the code maintained and keep track of changes in code; we used \href{https://git-scm.com/}{git} as a version control system
 (VCS). The repository can be found at \href{https://github.com/axilos22/rescuer_project}{rescuer\_project}.

\subsubsection{ROS}
\href{http://www.ros.org}{ROS} stands for Robotic Operating System, although is not technically an OS. ROS is a middleware that provides an easy to use communication platform for different software and hardware elements. ROS was a very useful tool to implement cooperation in our system, particularly the network communication among robots.

\subsubsection{Gmapping}
The \href{http://wiki.ros.org/gmapping}{Gmapping} ROS package is an implementation of the Simultanous Localization and Mapping (SLAM) algorithm. It builds a grid map from a laser scan, and localizes the robot from odometry data and features extracted from the environment.

\subsubsection{Navigation Stack}
The \href{http://wiki.ros.org/navigation}{Navigation} ROS package enables a mobile robot to avoid obstacles in the environment (based on sensor readings), make a plan to reach a goal and produces appropriate velocities to get there. It accomplishes this through two separate implementations. 

First it builds a global costmap of the perceived environment, and builds a global path from the current position to the goal based on Djisktra's algorithm, obtaining the lowest cost path. This part is platform independent. 

Secondly it builds a local costmap, taking into account close obstacles and the global path, and sends the command velocities to the platform. The local planner is  platform dependant, and has to be tuned for optimal performance.

\subsubsection{Tum Ardrone}
The \href{http://wiki.ros.org/tum_ardrone}{Tum Ardrone} ROS package allows us to control the AR Drone at a higher level. It implements three nodes: 

\begin{itemize}
\item State Estimation: Provides an estimate of the drone's position from the navigation data sent by the drone, the velocity commands and the tracking of key points in the front camera image. It uses Extended Kalman Filter (EKF) and Parallel Tracking and Mapping (PTAM) simultaneously for more accurate prediction.

\item Autopilot: Provides a PID controller for the AR Drone. The gains are already tuned correctly, but an aggressiveness parameter can be set to modify the response time.

\item Graphical User Interface: Monitors and controls the State Estimation and Autopilot nodes.

\end{itemize}

\subsubsection{Gazebo}
Gazebo is a well known and widely used simulator with a robust physics engine. 
Robot platforms like the TurtleBot and the AR Drone are already implemented, and new robots can be added using a SDF description. It also counts with different plugins  for robot, sensor, and environmental control. 

\subsubsection{Qt}
Qt is a well known GUI library. Is is used a large variety of applications from mobile phone interfaces to advanced customized programs. Qt is so popular that a module for ROS was
created called rqt (ROS Qt).


\subsection{Mobile base}
\begin{figure}[ht]	
\centering
\includegraphics[height=3cm]{turtlebot.png}
\caption{The Turtlebot}
\end{figure}

The mobile base used is a TurtleBot. TurtleBot is a low-cost, personal robot kit with open-
source software
provided by different partners mostly for educational activities.
Our mobile base was equipped with an ASUS Xtion PRO, which uses an infrared sensor and adaptive depth detection technology. Having such information is very interesting for 
navigation
purposes since the relative distance to object is easily accessible.


\subsection{Quadrotor}

The quadrotor used is a Parrot\textcopyright AR Drone v2\texttrademark it is a 4 propellers
drone which has a ROS compatible driver. It is also equipped with two cameras: one in front
and another in the bottom. The bottom camera can be used for visual tracking and the drone
has an on-board tracking system, allowing a decent tracking without latency issues.

\begin{figure}[ht]	
\centering
\includegraphics[height=2cm]{arDroneGpsEdition.png}
\caption{AR Drone v2}
\end{figure}

Additionally, some ROS libraries have been created to be compatible with the drone; we used
one of them called \href{"http://wiki.ros.org/tum_ardrone"}{"tum\_ardrone"} in parallel with
the driver package: \href{"https://github.com/AutonomyLab/ardrone_autonomy"}
{"ardrone\_autonomy"}.

\subsection{Cooperative behaviours}
As the two robots have to cooperate to perform their common goal, it was required to implement different cooperative behaviours
on the system. During the conceptualization of the project we established different
 behaviours that the system should be able to manage: decoupled, coupled and supervisor mode. Figure \ref{fig:Coop} displays a diagram of these behaviours.

\begin{figure}[ht]	
\includegraphics[height=7cm]{cooperativeModes.png}
\caption{Cooperative behaviours states map}
\label{fig:Coop}
\end{figure}

\subsubsection{Default mode}
The default mode is the one in which the system start. No particular behaviour is expected but the two
robots should establish connection and start exchanging their respective localization and status data.

\subsubsection{Coupled mode}
In this mode the two robots communicate to reach a user defined goal. The mobile base handles the path planning and navigation, and the drone follows. The drone tracks the mobile base either by requesting the mobile base location, or by visual tracking. From this point forward, if the mobile base moves, the drone remains on top and moves along with it.

\subsubsection{Decoupled mode}
This mode is the same as default mode in terms of behaviour. The only difference is that the default mode
is not accessed any more after start-up. When the system enters the decoupled mode, the two robots movements
are decoupled and can be separately controlled, either by tele-operation or any other method.

\subsubsection{Supervisor mode}
The supervisor mode is a semi-operated mode. In this mode, the drone will adopt a set of pre-defined
positions around the mobile base. For instance in the case of navigation, the drone will move 
among a semi-circle (or full circle) around the mobile platform, allowing the 
operator to have better awareness of the situation.

\begin{figure}[ht]	
\centering
\includegraphics[height=7cm]{assistanceBehavior.png}
\caption{Map of the assistance behaviour}
\end{figure}

Additional behaviours can be added into the supervisor mode to be even more effective, for
instance adopting a lateral camera position while the robot is grasping an object. This would
allow the operator to have a real-time visual feedback on the action. Another behaviour
could be ``forward scouting" allowing the drone to explore the area before engaging the 
mobile base.

\section{Work achieved}

\subsection{Simulator implementation}
In order to debug and test the cooperative behaviours without having to worry about the safety of the drone, we implemented a simulation of the whole system in Gazebo.

The Turtlebot has plenty of support in the community, and did not present any major difficulties to use it in the simulation. On the other hand, the AR Drone's simulator package (\verb!tum_sim!) was made originally for the Hydro version of ROS, one previous to the one we are using (Indigo). It was required to do some slight modifications to the SDF description files to make it compatible with our system.

Once we had each robot working separately on Gazebo, we had to be careful with topic and frame names when we launched them together. The namespace `quadrotor' was used to launch the drone model and related nodes. Changing the topic names caused the simulated drone to become unstable. The \verb!tum_sim! gazebo plugin had to be modified directly in the source code, since it was not possible to remap the topic names related to sensor information. The simulated drone behaved in a similar way to the real drone once the correct information was being received.

\subsection{Individual Robot Capabilities}
In order to have smart behaviours, such that the operator does not need to be concerned with basic commands, some autonomous capabilities were implemented on both robotic platforms. Mainly autonomous navigation of unknown environments for the mobile base, and tracking, both a visual pattern and a given point, for the drone.

\subsubsection{Mobile Base}
Autonomous navigation of unknown environments was implemented with the Turtlebot.

The Gmapping ROS package was used to create a map of the environment as the Turtlebot travels through it, and to provide more robust localization within this created map. In Figure \ref{fig:gmapping} we can see an example of a map produced by Gmapping. This map and the calculated location are then used by the Navigation stack.


\begin{figure}[ht!]%
	\centering
    \subfloat[Gazebo World]{{\includegraphics[height=3.4cm]{gazebo.png} }}  
    \qquad  
    \subfloat[Resulting Map]{{\includegraphics[height=3.4cm]{gmapping.png}}}
    \caption{Map produced by Gmapping}
    \label{fig:gmapping}
\end{figure}

Navigating through the environment was done through the Navigation ROS package, allowing the robot to reach a defined goal while avoiding obstacles. A special costmap plugin was used ( \href{https://github.com/MonicaArias/voronoi_navigation}{voronoi\_navigation} )  , which creates a Voronoi diagram of the map given by Gmapping. This allows the system to prioritize the path with the most clearance between obstacles. We can see in Figure \ref{fig:globalPath} an example of the planned path from the starting point [0,0] to the goal [5,3]. The path is highlighted in green, the obstacles (in yellow) are surrounded by a buffer area where the robot might collide; and the blue lines are the Voronoi diagram where the robot tries to go on.

\begin{figure}[h!]
	\centering
    \includegraphics[width=8cm]{globalplan53.png}
    \caption{Global path (in green) produced by the Navigation package}
    \label{fig:globalPath}
\end{figure}

One of the defined cooperative behaviours is that the drone follows the mobile base.  The extra clearance provided fullfills two roles: increased safety for the mobile base, since it is less likely to hit an obstacle when turning a corner; and it allows the drone enough space to not bump into walls. 


\subsubsection{Quadrotor}
We implemented two modes of tracking for the drone, visual and point tracking.

For visual tracking we used 
the on-board control algorithm of the drone. This had many advantages: 
\begin{itemize}
\item The implementation was facilitated: activating the tracking on the drone can be done by 
setting a few parameters to different values. Then additional parameters can be tuned to
obtain a more or less robust tracking.

\item The delay in control is reduced: although we don't have full control on the control
loop, the delay between sending commands and command application is very short. According to
the average ping of the drone, the delay can vary from 50~ms to 3000~ms. In a control loop,
such fluctuations can make the system unstable.

\item Monitoring of the environment is possible: the drone is only capable of broadcasting one camera feed at a time. Using the on-board tracking means that it is not necessary to broadcast the bottom camera, giving freedom to the user to choose which camera to visualize depending on the task at hand.

\end{itemize}


\begin{figure}[ht]
	\centering
    \includegraphics[height=4cm]{visualTracking.png}
    \caption{AR Drone tracking a visual pattern}
    \label{fig:visualTracking}
\end{figure}


Unfortunately the simulation of the drone does not include this on-board tracking. To accurately represent the real system we used the \verb!cmvision! ROS package to detect and track the top of the Turtlebot. We define the color to track and the \verb!cmvision! package provides us the image location of the detected blobs with this color. The \textit{quad\_blob\_tracker} node we created takes this position and commands the drone to keep the blob centred. The top of the Turtlebot is black, as well as other elements in the simulated world, but the tracking proved to be reliable when the Turtlebot is navigating to a defined goal.


\begin{figure}[ht!]%
	\centering
    \subfloat[Drone's bottom camera view]{{\includegraphics[height=3.4cm]{simTracking.png} }}  
    \qquad  
    \subfloat[Drone's position]{{\includegraphics[height=3.4cm]{simTracking2.png}}}
    \caption{AR Drone tracking the top of the Turtlebot}
    \label{fig:simTracking}
\end{figure}


The visual tracking allows the drone to follow the mobile base when it is in line of sight. In order to get to the mobile base from another location or to position the drone to supervise a task, we implemented a goto command with the help of the \verb!tum_ardrone! package.

As explained previously, the \verb!tum_ardrone! package provides a PID controller and a position estimation, which the controller takes as the current position of the drone. On the real drone it is necessary to have various keypoints on the front camera image to avoid drifting, since PTAM is used to estimate the position.

These keypoints are not necessary in the simulation, the Extended Kalman Filter is accurate enough, only the estimated height of the drone is not very reliable, and causes the platform to drift upwards. We created an intermediary node, \textit{replace\_z}, to take in the position estimation and replace the height element with the ground truth information provided by gazebo. This proved to stabilize the drone more robustly.

Once the drone is capable to go to a commanded position we can track a reference frame. In Figure \ref{fig:posT} we can see the drone approaches the tracked frame.

\begin{figure}[ht!]%
	\centering
    \subfloat[]{{\includegraphics[height=3cm]{frame_tracking3.png}}}  
    \qquad  
    \subfloat[]{{\includegraphics[height=3cm]{frame_tracking2.png} }}  
    \caption{Position tracking}
    \label{fig:posT}
\end{figure}

\begin{figure}[ht!]%
	\centering
    \subfloat[Version 1.0]{{\includegraphics[width=10cm]{guiSketch.png} \label{fig:gui1}}}  
    \qquad  
    \subfloat[Version 2.0]{{\includegraphics[width=10cm]{guiV2.png} }}  
    \qquad  
    \subfloat[Version 3.0]{{\includegraphics[width=10cm]{guiV3.png} \label{fig:gui3}}}  
    \caption{GUI development}
    \label{fig:gui}
\end{figure}

\subsection{GUI programming}
Programming the GUI was facilitated by the rqt package. Setting up the interface was, however,
 a complicated task since the tutorial of rqt is not exactly finished and omitted crucial set-up details. The rqt package creates a ROS nodelet which allows access to
all the ROS useful features: subscriber, publisher, message types, etc.

Then this nodelet acts like a Qt blanc page, allowing us to program the GUI we want. It is 
important to note that the QWidgets and the ROS services run on a different thread, implying some limitations because a ROS Callback
\href{http://wiki.ros.org/rqt/Tutorials/Writing\%20a\%20C\%2B\%2B\%20Plugin}
{cannot access a Qt element}.

With the first test widget operational, the development of the GUI started based on a first sketch, shown on Figure \ref{fig:gui1}. The development of the GUI followed the features we were
testing on the physical robots.

In Figure \ref{fig:gui3} we can see the final version of the GUI. It contains the following elements:

\begin{itemize}

	\item Mobile Base Control - Located on the left hand side panel
	\begin{itemize}
		\item Mobile base camera
		\item Position provided by the \textit{position\_rescuer} node, that transforms the reference frame from Gmapping into a point
		\item Goal to be sent to the Navigation package
		
	\end{itemize}
	
	\item Drone Control - Located on the right side of the panel
	\begin{itemize}
		\item Battery level
		\item Drone camera, either bottom or front camera, can be swithced with the 'SwitchCam' button below
		\item Teleop for both robots
		\begin{itemize}
			\item For the mobile base:
			\begin{itemize}
				\item "W" is forward, "X" is backwards
				\item "D" is turn right, "A" is turn left			
			\end{itemize}

			\item For the drone:
			\begin{itemize}
				\item Arrow up is forward, arrow down is backwards
				\item Arrow right is move to the right, arrow left is move to the left (there is no rotation because the state estimation depends on a fixed viewpoint)
				\item Page up is upwards, page down is downwards
			\end{itemize}	
		\end{itemize}

		\item Drone commands, including take off and landing
		\item Autopilot commands, to go to a given point with the \verb!tum_ardrone! package	
	\end{itemize}
	
	\item Behaviour mode, to be able to select between decoupled, coupled and supervised modes
	
	\item Console, to display information and debugging messages to the screen


\end{itemize}






\begin{figure}[ht]	
\centering
\includegraphics[width=14cm]{communicationArchitecture.png}
\caption{GUI Communication architecture}
\end{figure}


\subsection{Coupled mode interactions}

For the most part both robots cooperate without direct communication. The drone provides a better point of view of what the mobile base is doing, and  the human operator functions as the missing link to achieve one common goal. However, in the coupled mode both robots are expected to coordinate between them to arrive to a given point.  

The diagram of the implementation can be seen in Figure \ref{fig:coupled}. The mobile base, in this case a Turtlebot, handles the path planning and navigation part of the problem. As the mobile base moves toward the goal it broadcast a frame 1 meter above its location. 

The drone has to figure out how to follow the mobile base. If it can detect the mobile base visually, it will do visual tracking. And if the mobile base is out of sight the drone will send a message to the mobile base asking for a point to go to. 

When the mobile base receives this request it performs the frame transformation from the frame it is broadcasting above itself, to the odometry frame of the drone; and sends this point to the drone's PID controller.

\begin{figure}[ht]	
\centering
\includegraphics[width=12cm]{communication.png}
\caption{Coupled mode behaviour}
\label{fig:coupled}
\end{figure}




\section{Experiments and result analysis}
\subsection{Real life testing}
\subsubsection{Experiment 1}
The first test was to tele-operate the drone on top of the turtlebot, and let the on board control
manage the tracking.

\begin{figure}[ht!]%
	\centering
    \subfloat[]{{\includegraphics[height=3cm]{test1.png}}}  
    \qquad  
    \subfloat[]{{\includegraphics[height=3cm]{test2.png} }}  
    \qquad  
    \subfloat[]{{\includegraphics[height=3cm]{test3.png}}}       
    \qquad  
    \subfloat[]{{\includegraphics[height=3cm]{test4.png}}}       
    \qquad  
    \subfloat[]{{\includegraphics[height=3cm]{test5.png}}}  
    \caption{AR Drone on board tracking}
    \label{fig:exp1}
\end{figure}

The test was successful and no trouble arose during this phase. Indeed, tele-operating the drone is 
safe since we have total control over it's motion.
In Figure \ref{fig:exp1} we can see some screen shots of the recorded experiment. In a) the drone takes off, in b) it is teleoperated on top of the Turtlebot, and the tracking starts in c); the Turtlebot stops in d) and the drone lands in e). The tracking is precise enough that it is able to land on the mobile base.

As expected, the on board controller managed to keep the drone stable while still tracking the mobile base 
movement.  However, with further testing we noted that this controller does not re-center the drone: if the drone has a static
error, the tracker won't compensate it: the drone will remain shifted.

\subsubsection{Experiment 2}
The second test was to perform the same operation but using the auto-pilot of the \verb!tum_ardrone!
package to place the drone on top of the mobile base.

\begin{figure}[ht!]%
	\centering
    \subfloat[]{{\includegraphics[height=2.8cm]{test2_1.png}}}      
    \qquad  
    \subfloat[]{{\includegraphics[height=2.8cm]{test2_22.png}}}       
    \qquad  
    \subfloat[]{{\includegraphics[height=2.8cm]{test2_3.png}}}       
    \qquad  
    \subfloat[]{{\includegraphics[height=2.8cm]{test2_4.png}}}  
    \qquad  
    \subfloat[]{{\includegraphics[height=2.8cm]{test2_5.png}}}  
    \caption{AR Drone on board tracking}
    \label{fig:exp2}
\end{figure}



The experiment was not a total success, the auto-pilot relies a lot on the visual camera calibration to 
localize (having enough keypoint at take off), and this lead to erratic behaviour and static drifting.
In Figure \ref{fig:exp2} we showcase one of the experiments. In a) the drone takes off, in b) the drone gets to the desired position [0.5,0,0.7]. There is drifting and in c) the drone starts tracking the Turtlebot. The tracking finishes in d) and the drone lands on e). In this experiment the autopilot did get the drone on the desired position but then drifted away. 


We re-tried performing the experiment and again, the same stability issues happened. After a few tests we 
saw very different behaviours: sometimes the auto-pilot would manage perfectly to go to position without any
drifting and sometimes the drone would drift away dramatically. In order to minimize this problem, we 
automated the take off procedure to have the best possible camera calibration during take-off. This 
allowed to reduce the erratic behaviour but didn't entirely solved the problem: erratic behaviour can happen
especially of the front camera does not find enough visual keypoints.

\subsection{Simulation testing}
\subsubsection{Experiment 3}
In order to verify the tracking on the coupled mode, we implemented this set up in the Gazebo simulator. 
In the simulation the odometry frame of the robots begin where the robots appear, we set the Turtlebot to begin at [0,0] on the map and the drone at [-1, -1]. Therefore, we can relate both robots position by defining the transformation between them as this fixed distance, with the help of the tf ROS package.

Since we know the position of both robots in the simulated world, we were able to graph the position error in Figure \ref{fig:exp3}, the tracking mode is in light blue (1 is visual tracking and -1 position tracking); the error in x is in red and the error in y is in dark blue. 

At 58s the coupled mode is activated, we can see the drone has the Turtlebot on its line of sight and it reaches it very quickly. There is some oscillation while the drone tries to center the Turtlebot on the camera view and overshoots. At 70s the Turtlebot is given [3,3] as the goal point and at 90s it reaches the goal. From the error plot we can conclude the drone never lost sight of the Turtlebot during this trajectory,  and the error was kept under 0.8m.  Screen shots of the simulation can be seen in \ref{fig:exp3a}.

There are other goal points, but now we will focus on the final portion of the test. At 140s the Turtlebot is at [3,3], the drone is visual tracking mode, and we give the goal [0,0]. At 145s the drone hits an obstacle and cannot follow the Turtlebot further, it looses sight at 147s, and changes to position tracking control. The Turtlebot reaches the goal at 160s, and the drone gets there at 178s. Screen shots of the simulation can be seen in \ref{fig:exp3b}.

With this experiment we prove that the coupled mode behaviour works as intended. In fact the color blob tracking is very reliable, the drone had to bump into obstacles to loose track of the Turtlebot. 
The difference in tracking speed is significant, since the visual tracking tries to follow the Turtlebot very closely, but the position tracking is very slow. 

\begin{figure}[ht]	
\centering
\includegraphics[height=7cm]{error1video.png}
\caption{Position error on coupled mode}
\label{fig:exp3}
\end{figure}

\begin{figure}[ht]%
	\centering
    \subfloat[58s]{{\includegraphics[height=2.8cm]{exp3_1.png}}}      
    \qquad  
    \subfloat[65s]{{\includegraphics[height=2.8cm]{exp3_2.png}}}       
    \qquad  
    \subfloat[75s]{{\includegraphics[height=2.8cm]{exp3_3.png}}}       
    \qquad  
    \subfloat[85s]{{\includegraphics[height=2.8cm]{exp3_4.png}}}  
    \qquad  
    \subfloat[90s]{{\includegraphics[height=2.8cm]{exp3_5.png}}}  
    \caption{Color blob tracking}
    \label{fig:exp3a}
\end{figure}

\begin{figure}[ht]%
	\centering
    \subfloat[140s]{{\includegraphics[height=2.8cm]{exp3_6.png}}}      
    \qquad  
    \subfloat[145s]{{\includegraphics[height=2.8cm]{exp3_7.png}}}       
    \qquad  
    \subfloat[147s]{{\includegraphics[height=2.8cm]{exp3_8.png}}}       
    \qquad  
    \subfloat[160s]{{\includegraphics[height=2.8cm]{exp3_9.png}}}  
    \qquad  
    \subfloat[178s]{{\includegraphics[height=2.8cm]{exp3_10.png}}}  
    \caption{Position tracking}
    \label{fig:exp3b}
\end{figure}



\subsection{Improvement and future work}
\subsubsection{Graphical user interface}
Improving the current GUI can be made by adding more dynamic elements into it. The current 
problem is
that it displays lots of information in a ``raw" manner. The global design of the GUI could be 
revisited in
order to present all information  in a coherent and clear way.

\subsubsection{Cooperative behaviour}
We have presented many different cooperative behaviour for this system, however we only had 
time to implement one of them. It would be interesting now that the two robots are under 
control, to implement
different cooperative behaviours, test their stability and eventually keep the most robust and 
effective
one.

\subsubsection{Simulation}
As the simulator does not reproduce accurately the physical behaviour of the drone, having a 
more detailed AR Drone model would help to test stability and control issues, without having 
to use the real physical system.

\clearpage
\section{Conclusion}
Cooperative robotics is the next step of evolution in robotic systems.
Their versatility and relative architectural simplicity make them both very useful while
keeping a design simple enough to be maintainable.

The general lack of GUI in robotic systems is also a problem that have to be addressed. 
Robot programmers generally focus on software and middle-ware accurate programs while 
overlooking the final user activities which might be greatly facilitated by a clear and 
well-designed GUI.
By using both good middle-ware libraries (like ROS) combined with good higher level
software (as Qt) programs for robots can get both easier to use and provide a better 
feedback.

Implementing cooperative behaviours into (tele-operated) multi-robots system allows to release 
the operator task load. This permit to focus on the end-goal of the mission while not dealing 
with situational issues such as camera positioning or sensor feedback accuracy.
\bibliography{bib}
\bibliographystyle{abbrv}
\end{document}
